{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import morfeusz2\n",
    "from elasticsearch import Elasticsearch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, TextStreamer\n",
    "import torch\n",
    "from typing import List\n",
    "from langchain_core.documents.base import Document\n",
    "import spacy\n",
    "from sentence_transformers import util\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# https://github.com/stopwords-iso/stopwords-pl\n",
    "# touch /home/m.wnuczynski/nltk_data/corpora/stopwords/polish\n",
    "with open('stopwords-pl.txt', 'r', encoding='utf-8') as file:\n",
    "    polish_stopwords = file.read().splitlines()\n",
    "    \n",
    "stopwords.words('polish').extend(polish_stopwords)\n",
    "morf = morfeusz2.Morfeusz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wczytywanie danych z drzewa strony wydzialu ETI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_recursive(directory):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for name in files:\n",
    "            if '.txt' in name:\n",
    "                documents.append(str(os.path.join(root, name)))\n",
    "    return documents\n",
    "\n",
    "files = list_files_recursive(\"/home/nukeemann/github/scrapper_mgr/etipg/eti.pg.edu.pl/\")\n",
    "raw_documents = 0\n",
    "for file in files:\n",
    "    loader = TextLoader(file)\n",
    "    if not raw_documents:\n",
    "        raw_documents = loader.load()\n",
    "    else:\n",
    "        raw_documents += loader.load()\n",
    "\n",
    "print(f\"Załadowano {len(raw_documents)} dokumentów\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podzielenie dokumentów na chunki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Chunk size you want\n",
    "    chunk_overlap=100 # Overlap between chunks\n",
    ")\n",
    "\n",
    "# Function to split documents and retain them as list of Langchain Document objects\n",
    "def split_documents(documents):\n",
    "    split_docs = []\n",
    "    for doc in documents:\n",
    "        chunks = text_splitter.split_text(doc.page_content)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Create a new Document for each chunk\n",
    "            chunk_doc = Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    **doc.metadata,  # retain original metadata\n",
    "                }\n",
    "            )\n",
    "            split_docs.append(chunk_doc)\n",
    "    return split_docs\n",
    "\n",
    "# Split the documents\n",
    "split_raw_document_list = split_documents(raw_documents)\n",
    "print(f\"Podzielono {len(raw_documents)} dokumentów na {len(split_raw_document_list)} chunków\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing:\n",
    "- Usuniecie znaczników HTML\n",
    "- Usuniecie pustych linii\n",
    "- Lematyzacja słów\n",
    "- Usunięcie polskich stopword-ów\n",
    "- Zastąpienie polskich znaków\n",
    "- Przeprowadzenie NER na dokumencie\n",
    "- Zmienienie tekstu na małą czcionke\n",
    "- Usunięcia drzewa katalogów ze ścieżki źródłowej dokumentu\n",
    "- Przeprowadzenie embedding-u na dokumentach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy model for Polish NER\n",
    "nlp_pl = spacy.load(\"pl_core_news_sm\")\n",
    "\n",
    "# Load tokenizer and embedder\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Voicelab/sbert-base-cased-pl\")\n",
    "embedding_model = AutoModel.from_pretrained(\"Voicelab/sbert-base-cased-pl\")\n",
    "\n",
    "# Named Entity Recognition in document\n",
    "def ner(text):\n",
    "    ner_res = nlp_pl(text)\n",
    "    found_entities = []\n",
    "    for ent in ner_res.ents:\n",
    "        found_entities.append({'entity_name': ent.text, 'entity_type': ent.label_})\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_entities = list({frozenset(entity.items()): entity for entity in found_entities}.values())\n",
    "    return unique_entities\n",
    "\n",
    "# Replace polish letters\n",
    "def replace_polish_letters(text): \n",
    "    polish_to_english = { 'ą': 'a', 'ć': 'c', 'ę': 'e', 'ł': 'l', 'ń': 'n', 'ó': 'o', 'ś': 's', 'ź': 'z', 'ż': 'z', 'Ą': 'A', 'Ć': 'C', 'Ę': 'E', 'Ł': 'L', 'Ń': 'N', 'Ó': 'O', 'Ś': 'S', 'Ź': 'Z', 'Ż': 'Z' } \n",
    "    for polish_char, english_char in polish_to_english.items(): \n",
    "        text = text.replace(polish_char, english_char) \n",
    "    return text\n",
    "\n",
    "# Clean text, remove polish words and extract entities\n",
    "def document_cleaner(doc: Document):\n",
    "    text = doc.page_content\n",
    "    \n",
    "    # Remove any leftover HTML tags\n",
    "    clean_html = re.compile('<.*?>')\n",
    "    text = re.sub(clean_html, '', text)\n",
    "    \n",
    "    # Remove new lines\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_words = []\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            analyses = morf.analyse(word)\n",
    "            if analyses and len(analyses) == 1:\n",
    "                lemma = analyses[0][2][1]\n",
    "            else:\n",
    "                lemma = word\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during analysis of word '{word}': {e}\")\n",
    "            lemma = word\n",
    "\n",
    "        lemmatized_words.append(lemma)\n",
    "    text_uni = ' '.join(lemmatized_words)\n",
    "    \n",
    "    # Remove Polish stopwords\n",
    "    stop_words = set(stopwords.words('polish'))\n",
    "    text_uni = ' '.join([word for word in text_uni.split() if word not in stop_words])\n",
    "    \n",
    "    # Replace polish letters\n",
    "    text_uni = replace_polish_letters(text_uni)\n",
    "    \n",
    "    # Remove unnecessary whitespace\n",
    "    cleaned_text = \" \".join(text_uni.split())\n",
    "\n",
    "    # Named Entity Recognition extraction in Polish\n",
    "    entities = ner(cleaned_text)\n",
    "    \n",
    "    # Convert page_content to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "\n",
    "    # Remove local directory path from source\n",
    "    source_path = ''.join(doc.metadata['source'].split('/etipg/')[1:])\n",
    "    \n",
    "    return {'source_text': text, 'cleaned_text': cleaned_text, 'entities': entities, 'source': source_path}\n",
    "\n",
    "# Embedd the document\n",
    "def embedd_doc(preprocessed_doc):\n",
    "    inputs = tokenizer(preprocessed_doc['cleaned_text'], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = embedding_model(**inputs)\n",
    "\n",
    "    embedding = outputs.pooler_output.detach().numpy()[0]\n",
    "    return {'source': preprocessed_doc['source'], 'text_embedded': embedding, 'cleaned_text': preprocessed_doc['cleaned_text'], 'source_text': preprocessed_doc['source_text'], 'entities': preprocessed_doc['entities']}\n",
    "\n",
    "# Process documents\n",
    "def process_documents(documents: List[Document]):\n",
    "    # Preprocess each document\n",
    "    preprocessed_docs = [document_cleaner(doc) for doc in documents]\n",
    "    \n",
    "    # Embedd the preprocessed documents\n",
    "    embedded_docs = [embedd_doc(doc) for doc in preprocessed_docs]\n",
    "    \n",
    "    # Create index structure\n",
    "    embedded_docs_dict = []\n",
    "    for doc in embedded_docs:\n",
    "        embedded_docs_dict.append({\n",
    "            'source': doc['source'],\n",
    "            'embedding': doc['text_embedded'],\n",
    "            'source_text': doc['source_text'],\n",
    "            'cleaned_text': doc['cleaned_text'],\n",
    "            'entities': doc['entities']\n",
    "        })\n",
    "    \n",
    "    return embedded_docs_dict\n",
    "\n",
    "\n",
    "embedded_docs_dict = process_documents(split_raw_document_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stworzenie bazy danych ElasticSearch i załadowanie dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all documents\n",
    "#es_client.indices.delete(index='mgr_test_1_embedded', ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define low-level ElasticSearch client\n",
    "es_client = Elasticsearch(\n",
    "    cloud_id=\"\",\n",
    "    api_key=\"\"\n",
    ")\n",
    "\n",
    "def index_single_document(index_name, document):\n",
    "    response = es_client.index(\n",
    "        index=index_name,\n",
    "        document=document\n",
    "    )\n",
    "    print(f\"Document indexed: {response['_id']}\")\n",
    "\n",
    "for embedded_doc in embedded_docs_dict:\n",
    "    index_single_document(\"mgr_test_1_embedded\", embedded_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stworzenie zapytania i wydobycie adekwatnych dokumentów z bazy danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedd_query(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs)\n",
    "    return outputs.pooler_output.detach().numpy()[0]\n",
    "\n",
    "def process_query(query):\n",
    "    # Remove new lines\n",
    "    text = query.replace('\\n', ' ')\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_words = []\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            analyses = morf.analyse(word)\n",
    "            if analyses and len(analyses) == 1:\n",
    "                lemma = analyses[0][2][1]\n",
    "            else:\n",
    "                lemma = word\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during analysis of word '{word}': {e}\")\n",
    "            lemma = word\n",
    "        lemmatized_words.append(lemma)\n",
    "    text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    # Remove Polish stopwords\n",
    "    stop_words = set(stopwords.words('polish'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # Replace polish letters\n",
    "    text = replace_polish_letters(text)\n",
    "\n",
    "    # Look for entities\n",
    "    print(text)\n",
    "    query_entities = ner(text)\n",
    "    print(query_entities)\n",
    "\n",
    "    # COnvert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove unnecessary whitespace\n",
    "    cleaned_text = \" \".join(text.split())\n",
    "\n",
    "    # Embedd query\n",
    "    query_embedding = embedd_query(cleaned_text)\n",
    "\n",
    "    return {'source_text': query, 'cleaned_text': cleaned_text, 'embedding': query_embedding, 'entities': query_entities}\n",
    "\n",
    "\n",
    "# Create and clean query\n",
    "query_text = \"habilitacja w dyscyplina AEEiTK prowadzona przez Miranda Rogoda-Zawiasa\"\n",
    "query = process_query(query_text)\n",
    "\n",
    "# Define the Elasticsearch index name\n",
    "index_name = \"mgr_test_1_embedded\"\n",
    "\n",
    "# Construct the search query using script_score for cosine similarity\n",
    "search_query = {\n",
    "    \"size\": 5,  # Set the number of results you want to retrieve\n",
    "    \"query\": {\n",
    "        \"script_score\": {\n",
    "            \"query\": {\"match_all\": {}},  # Retrieves all documents to score\n",
    "            \"script\": {\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, 'embedding')\",\n",
    "                \"params\": {\"query_vector\": query['embedding']}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Execute the search\n",
    "retrieved_docs = es_client.search(index=index_name, body=search_query)\n",
    "\n",
    "# Process the response and print results\n",
    "for i, hit in enumerate(retrieved_docs['hits']['hits']):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Source: {hit['_source'].get('source')}\")\n",
    "    print(f\"Content: {hit['_source'].get('source_text')}\")\n",
    "    print(f\"Embeddings: {hit['_source'].get('embedding')}\")\n",
    "    print(f\"Entities: {hit['_source'].get('entities')}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reranker zliczający powtarzające się wyrazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve initial results\n",
    "documents = retrieved_docs['hits']['hits']\n",
    "\n",
    "# Naive re-ranking by a simple heuristic: count of matched keywords\n",
    "def naive_rerank(docs, query_keywords):\n",
    "    for doc in docs:\n",
    "        source_text = doc['_source']['cleaned_text']\n",
    "        score_adjustment = sum(1 for keyword in query_keywords if keyword in source_text)\n",
    "        doc['_score'] += score_adjustment  # Adjust score\n",
    "    return sorted(docs, key=lambda d: d['_score'], reverse=True)\n",
    "\n",
    "# Example usage\n",
    "query_keywords = query['cleaned_text'].split()\n",
    "reranked_docs_naive = naive_rerank(documents, query_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, hit in enumerate(reranked_docs_naive):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Source: {hit['_source'].get('source')}\")\n",
    "    print(f\"Content: {hit['_source'].get('source_text')}\")\n",
    "    print(f\"Embeddings: {hit['_source'].get('embedding')}\")\n",
    "    print(f\"Entities: {hit['_source'].get('entities')}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reranker zliczający powtarzające się entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_entities(documents, query):\n",
    "    query_entity_names = [item['entity_name'] for item in query['entities']]\n",
    "    for doc in documents:\n",
    "        matching_entities_score = 0\n",
    "        doc_entities = doc['_source'].get('entities', [])\n",
    "        \n",
    "        # Calculate score based on matched entities\n",
    "        for entity in doc_entities:\n",
    "            if entity['entity_name'] in query_entity_names:\n",
    "                matching_entities_score += 1\n",
    "        \n",
    "        # Adjust the score by adding the matching entity score\n",
    "        doc['_score'] += matching_entities_score\n",
    "    \n",
    "    # Sort documents based on the adjusted score\n",
    "    return sorted(documents, key=lambda d: d['_score'], reverse=True)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "reranked_docs_advanced = rerank_entities(documents, query)\n",
    "\n",
    "for i, hit in enumerate(reranked_docs_advanced):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Source: {hit['_source'].get('source')}\")\n",
    "    print(f\"Content: {hit['_source'].get('source_text')}\")\n",
    "    print(f\"Embeddings: {hit['_source'].get('embedding')}\")\n",
    "    print(f\"Entities: {hit['_source'].get('entities')}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reranker wyliczający wartości semantyczne między zapytaniem a dokumentami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert embeddings to tensors\n",
    "document_embeddings_t = [torch.tensor(doc['_source']['embedding']) for doc in documents]\n",
    "query_embedding_t = torch.tensor(query['embedding']).unsqueeze(dim=0)\n",
    "\n",
    "# Compute similarity scores\n",
    "reranked_docs_semantic = []\n",
    "scores = util.semantic_search(query_embedding_t, document_embeddings_t, top_k=len(document_embeddings_t))\n",
    "\n",
    "for score in scores[0]:\n",
    "    doc_index = score['corpus_id']\n",
    "    doc = documents[doc_index]\n",
    "    doc['_score'] = score['score']\n",
    "    reranked_docs_semantic.append(doc)\n",
    "\n",
    "# Sort documents by the updated similarity score\n",
    "reranked_docs_semantic = sorted(reranked_docs_semantic, key=lambda x: -x['_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, hit in enumerate(reranked_docs_semantic):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Source: {hit['_source'].get('source')}\")\n",
    "    print(f\"Content: {hit['_source'].get('source_text')}\")\n",
    "    print(f\"Embeddings: {hit['_source'].get('embedding')}\")\n",
    "    print(f\"Entities: {hit['_source'].get('entities')}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zdefiniowanie łańcucha RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zdefiniowanie Modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"speakleash/Bielik-11B-v2.3-Instruct\"\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zdefiniowanie funkcji wywołującej zapytanie do modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG system\n",
    "def generate_answer(query, docs):\n",
    "    messages = []\n",
    "    \n",
    "    # Assmeble the prompt\n",
    "    messages.append({\"role\": \"system\", \"content\": \"Jesteś asystentem AI, który odpowiada na pytania korzystając z dostarczonego kontekstu. Twoje odpowiedzi powinny być krótkie, precyzyjne i w języku polskim.\"})\n",
    "    for doc in docs[:3]:\n",
    "        messages.append({\"role\": \"system\", \"content\": f\"Dokument ze strony {doc['_source'].get('source').replace('context.txt', '')}: {doc['_source'].get('source_text')}\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "    for message in messages:\n",
    "        print(message)\n",
    "\n",
    "    # Apply tags to the prompt\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate the answer\n",
    "    llm.generate(input_ids, streamer=streamer, max_new_tokens=1000, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Gdzie mogę się dowiedzieć informacji o postępowaniach w sprawie nadania stopnia doktora, oraaz z jakich specjalizacji można robić doktorat?\"\n",
    "generate_answer(query, reranked_docs_semantic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
